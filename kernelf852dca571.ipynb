{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom os import listdir\nfrom os.path import isfile, join\n\ntrain_images_path = '../input/siimacrpneumothoraxsegmentationzip-dataset/input/train/images/512/dicom'\ntrain_masks_path = '../input/siimacrpneumothoraxsegmentationzip-dataset/input/train/images/512/mask'\ntrain_csv = '../input/siimacrpneumothoraxsegmentationzip-dataset/input/train/train-rle.csv'\n\ntest_images_path = '../input/siimacrpneumothoraxsegmentationzip-dataset/input/test/images/512/dicom'\ntest_masks_path = '../input/siimacrpneumothoraxsegmentationzip-dataset/input/test/images/512/mask'\n\ntrain_images = [f for f in listdir(train_images_path) if isfile(join(train_images_path,f))]\ntrain_images.sort()\ntrain_masks = [f for f in listdir(train_masks_path) if isfile(join(train_masks_path,f))]\ntrain_masks.sort()\ntest_images = [f for f in listdir(test_images_path) if isfile(join(test_images_path,f))]\ntest_images.sort()\ntest_masks = [f for f in listdir(test_masks_path) if isfile(join(test_masks_path,f))]\ntest_masks.sort()\n\ntrain_label = pd.read_csv(train_csv)\ntrain_label.head()\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random, cv2\n\ntrains = list(zip(train_images,train_masks))\nrandom.shuffle(trains)\ntrain_images, train_masks = zip(*trains)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_images[0])\nprint(train_masks[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread(train_images_path+'/'+train_images[10],cv2.IMREAD_GRAYSCALE)\nimg2 = img/255.\nimg3 = img/img.max()\nimg4 = (img-img.min())/(img.max()-img.min())\nimg5 = (img-img.mean())/img.std()\nimg6 = img.copy()\ncv2.normalize(img,img6,0,1,cv2.NORM_MINMAX)\n\nimport matplotlib.pyplot as plt\n\n# Check if training data looks all right\nix = random.randint(0, len(train_images))\nf,ax = plt.subplots(1,6,figsize=(12,5))\nax[0].imshow(img,cmap='gray')\nax[1].imshow(img2,cmap='gray')\nax[2].imshow(img3,cmap='gray')\nax[3].imshow(img4,cmap='gray')\nax[4].imshow(img5,cmap='gray')\nax[5].imshow(img6,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf\nimport keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_WIDTH = 512\nIMG_HEIGHT = 512\nIMG_CHANNELS = 1\n\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, x, y, batch_size=32, dim=(512,512), n_channels=1, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.x = x\n        self.y = y\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.x) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        x_list = [self.x[k] for k in indexes]\n\n        # Generate data\n        x, y = self.__data_generation(x_list)\n\n        return x, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.x))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, x_list):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        x = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n        y = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n\n        # Generate data\n        for i, x in enumerate(x_list):\n            # Store sample\n            x[i,] = np.expand_dims(cv2.imread(train_images_path+'/'+x,cv2.IMREAD_GRAYSCALE),axis=2)\n            x[i,] = x[i,] / 255.\n            # Store class\n            y[i,] = np.expand_dims(cv2.imread(train_masks_path+'/'+x,cv2.IMREAD_GRAYSCALE),axis=2)\n            y[i,] = y[i,] / 255.\n\n        return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'dim': (IMG_HEIGHT,IMG_WIDTH),\n          'batch_size': 64,\n          'n_channels': 1,\n          'shuffle': True}\n\ntrain_generator = DataGenerator(train_images, train_masks, **params)\ntest_generator = DataGenerator(test_images, test_masks, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Check if training data looks all right\nix = random.randint(0, len(train_images))\nf,ax = plt.subplots(1,2)\nax[0].imshow(np.squeeze(cv2.imread(train_images_path+'/'+train_images[ix])),cmap='gray')\nax[1].imshow(np.squeeze(cv2.imread(train_masks_path+'/'+train_masks[ix])),cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build U-Net model\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = Lambda(lambda x: x / 255) (inputs)\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[mean_iou])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nhistory_callback = model.fit_generator(generator=train_generator,\n                    validation_data=test_generator, verbose=1,\n                    epochs=100, use_multiprocessing=True, workers=6)\n\nacc_history = history_callback.history[\"acc\"]\nloss_history = history_callback.history[\"loss\"]\nval_acc_history = history_callback.history[\"val_acc\"]\nval_loss_history = history_callback.history[\"val_loss\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('acc.txt', 'w') as f:\n    for item in acc_history:\n        f.write(\"%s\\n\" % item)\nwith open('loss.txt', 'w') as f:\n    for item in loss_history:\n        f.write(\"%s\\n\" % item)\nwith open('val_acc.txt', 'w') as f:\n    for item in val_acc_history:\n        f.write(\"%s\\n\" % item)\nwith open('val_loss.txt', 'w') as f:\n    for item in val_loss_history:\n        f.write(\"%s\\n\" % item)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}